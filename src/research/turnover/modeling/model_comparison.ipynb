{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitvenvvenv5ea2b9d028884ecdb1defa97f4866164",
   "display_name": "Python 3.6.9 64-bit ('.venv': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES_PATH = '../../../resources'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_reports(model):\n",
    "    report_paths = [f for f in Path(f'./{model}/report').iterdir()]\n",
    "\n",
    "    result = {}\n",
    "    for report_path in report_paths:\n",
    "        with open(report_path, 'r') as fin:\n",
    "            result[report_path.stem] = json.load(fin)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_reports():\n",
    "     models = [str(f) for f in Path('./').iterdir() if f.is_dir()]\n",
    "\n",
    "     result = {}\n",
    "     for model in models:\n",
    "         model_reports = get_model_reports(model)\n",
    "         for report_name in model_reports:\n",
    "             result[f'{model} {report_name}'] = model_reports[report_name]\n",
    "    \n",
    "     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reports = get_reports()"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Common report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df = pd.DataFrame(reports).T.set_index('Name')\n",
    "common_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df.sort_values(by='[Cleared Test] Log Loss')[['[Cleared Test] Log Loss']] # Cleared test model rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_df.sort_values(by='[Original Test] Log Loss')[['[Original Test] Log Loss']] # Original test model rating"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{RESOURCES_PATH}/dataset/turnover/label_encoder.pkl', 'rb') as fin:\n",
    "    le = pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cat_most_likely_mistakes(conf_mtx, cat):\n",
    "    total_mistakes = conf_mtx[cat].sum() - conf_mtx[cat][cat]\n",
    "    mistakes = np.concatenate((conf_mtx[cat][:cat], np.zeros(1), conf_mtx[cat][cat+1:]))\n",
    "    mistakes_norm = mistakes / total_mistakes\n",
    "    top_mistakes = sorted(enumerate(mistakes_norm), key=lambda i: i[1], reverse=True)[:3]\n",
    "\n",
    "    result = ''\n",
    "    for i in range(len(top_mistakes)):\n",
    "        result += f'    {i+1}. {le.inverse_transform([top_mistakes[i][0]])[0][:50]} - {round(top_mistakes[i][1] * 100, 2)}%\\n'\n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_confused(conf_mtx):\n",
    "    conf_mtx = np.array(conf_mtx)\n",
    "    cat_total = conf_mtx.sum(axis=1)\n",
    "    cat_correct = conf_mtx.diagonal()\n",
    "    cat_errors = cat_total - cat_correct\n",
    "    cat_errors_norm = cat_errors / cat_errors.sum()\n",
    "    cat_errors_in_cat_norm = cat_errors / cat_total\n",
    "    top_wrong = sorted(enumerate(cat_errors_norm), key=lambda i: i[1], reverse=True)[:10]\n",
    "\n",
    "    result = ''\n",
    "    for i in range(len(top_wrong)):\n",
    "        result += f'{i+1}. {le.inverse_transform([top_wrong[i][0]])[0][:50]} - {round(top_wrong[i][1] * 100, 2)}% ({round(cat_errors_in_cat_norm[top_wrong[i][0]] * 100, 2)}% in cat.)\\n'\n",
    "        result += get_cat_most_likely_mistakes(conf_mtx, top_wrong[i][0])\n",
    "        result += '\\n'\n",
    "    \n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./bert/confusion_matrix/base.json\", \"r\") as fin:\n",
    "    ft_bert_fc_conf_mtx = json.load(fin)\n",
    "\n",
    "with open(\"./mean_fc_nn/confusion_matrix/fasttext.json\", \"r\") as fin:\n",
    "    fasttext_fc_conf_mtx = json.load(fin)\n",
    "\n",
    "with open(\"./mean_fc_nn/confusion_matrix/bert.json\", \"r\") as fin:\n",
    "    bert_fc_conf_mtx = json.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cleared Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Fine-Tunned BERT FC': {'': get_most_confused(ft_bert_fc_conf_mtx['Cleared Test'])},\n",
    "    'fastText FC': {'': get_most_confused(fasttext_fc_conf_mtx['Cleared Test'])},\n",
    "    'BERT FC': {'': get_most_confused(bert_fc_conf_mtx['Cleared Test'])}\n",
    "})\n",
    "\n",
    "display(df.style.set_properties(**{\n",
    "    'text-align': 'left',\n",
    "    'white-space': 'pre-wrap'\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Original Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'Fine-Tunned BERT FC': {'': get_most_confused(ft_bert_fc_conf_mtx['Original Test'])},\n",
    "    'fastText FC': {'': get_most_confused(fasttext_fc_conf_mtx['Original Test'])},\n",
    "    'BERT FC': {'': get_most_confused(bert_fc_conf_mtx['Original Test'])}\n",
    "})\n",
    "\n",
    "display(df.style.set_properties(**{\n",
    "    'text-align': 'left',\n",
    "    'white-space': 'pre-wrap'\n",
    "}))"
   ]
  }
 ]
}