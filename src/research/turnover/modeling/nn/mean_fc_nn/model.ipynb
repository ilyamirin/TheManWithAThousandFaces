{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36964bitvenvvenv5ea2b9d028884ecdb1defa97f4866164",
   "display_name": "Python 3.6.9 64-bit ('.venv': venv)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from time import time\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate, Lambda, Bidirectional, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCES_PATH = '../../../../../resources'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING = 'BERT'\n",
    "ADDITIONAL_REPORT_METRICS = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NOMENCLATURE_LEN = {'BERT': 23, 'fastText': 17}[EMBEDDING]\n",
    "MAX_DESCRIPTION_LEN = {'BERT': 45, 'fastText': 30}[EMBEDDING]\n",
    "EMBEDDING_VEC_LEN = {'BERT': 768, 'fastText': 300}[EMBEDDING]\n",
    "\n",
    "MAX_EPOCHS = 300\n",
    "EARLY_STOP_PATIENCE = 15\n",
    "WORKERS = multiprocessing.cpu_count()-1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Move to commons\n",
    "\n",
    "def load_dfs():\n",
    "    train_df = pd.read_csv(f'{RESOURCES_PATH}/dataset/turnover/cleared_train.tsv', sep='\\t')\n",
    "    test_df = pd.read_csv(f'{RESOURCES_PATH}/dataset/turnover/cleared_test.tsv', sep='\\t')\n",
    "    original_test_df = pd.read_csv(f'{RESOURCES_PATH}/dataset/turnover/original_test.tsv', sep='\\t')\n",
    "\n",
    "    train_df.fillna('', inplace=True)\n",
    "    test_df.fillna('', inplace=True)\n",
    "    original_test_df.fillna('', inplace=True)\n",
    "\n",
    "    with open(f'{RESOURCES_PATH}/dataset/turnover/label_encoder.pkl', 'rb') as fin:\n",
    "        le = pickle.load(fin)\n",
    "\n",
    "    train_df.turnover = le.transform(train_df.turnover)\n",
    "    test_df.turnover = le.transform(test_df.turnover)\n",
    "    original_test_df.turnover = le.transform(original_test_df.turnover)\n",
    "\n",
    "    return train_df, test_df, original_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df, original_test_df = load_dfs()\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{RESOURCES_PATH}/cache/{EMBEDDING.lower()}_embedding_map.pkl', 'rb') as fin:\n",
    "    embedding_map = pickle.load(fin)\n",
    "\n",
    "embedding_map['description'][''] = []\n",
    "\n",
    "def to_vectors(df):\n",
    "    y = to_categorical(df.turnover)\n",
    "    x = [\n",
    "        pad_sequences([embedding_map['nomenclature'][i] for i in df.nomenclature], maxlen=MAX_NOMENCLATURE_LEN, dtype='float32'),\n",
    "        pad_sequences([embedding_map['description'][i] for i in df.description], maxlen=MAX_DESCRIPTION_LEN, dtype='float32')\n",
    "    ]\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = to_vectors(train_df)\n",
    "x_test, y_test = to_vectors(test_df)\n",
    "x_original_test, y_original_test = to_vectors(original_test_df)\n",
    "\n",
    "x_train[0].shape, x_train[1].shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomenclature_input = Input(shape=(MAX_NOMENCLATURE_LEN, EMBEDDING_VEC_LEN))\n",
    "nomenclature_mean_input = Lambda(lambda it: K.mean(it, axis=1))(nomenclature_input)\n",
    "\n",
    "nomenclature_branch = Dense(512, activation=\"relu\")(nomenclature_mean_input)\n",
    "nomenclature_branch = BatchNormalization()(nomenclature_branch)\n",
    "nomenclature_branch = Dropout(0.2)(nomenclature_branch)\n",
    "\n",
    "\n",
    "description_input = Input(shape=(MAX_DESCRIPTION_LEN, EMBEDDING_VEC_LEN))\n",
    "description_mean_input = Lambda(lambda it: K.mean(it, axis=1))(description_input)\n",
    "\n",
    "description_branch = Dense(512, activation=\"relu\")(description_mean_input)\n",
    "description_branch = BatchNormalization()(description_branch)\n",
    "description_branch = Dropout(0.2)(description_branch)\n",
    "\n",
    "\n",
    "common_branch = Concatenate(axis=1)([nomenclature_branch, description_branch])\n",
    "\n",
    "\n",
    "common_branch = Dense(512, activation=\"relu\")(common_branch)\n",
    "common_branch = BatchNormalization()(common_branch)\n",
    "common_branch = Dropout(0.2)(common_branch)\n",
    "\n",
    "common_branch = Dense(len(train_df.turnover.unique()), activation='softmax')(common_branch)\n",
    "\n",
    "\n",
    "model = Model(inputs=[nomenclature_input, description_input], outputs=common_branch)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(f'{RESOURCES_PATH}/model_checkpoint/turnover/nn/mean_fc_nn/').mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_started_at = time()\n",
    "\n",
    "fit_report = model.fit(\n",
    "    x_train, y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=MAX_EPOCHS,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        EarlyStopping(monitor='val_loss', patience=EARLY_STOP_PATIENCE),\n",
    "        ModelCheckpoint(f'{RESOURCES_PATH}/model_checkpoint/turnover/nn/mean_fc_nn/model.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
    "    ],\n",
    "    workers=WORKERS\n",
    ")\n",
    "\n",
    "training_time_sec = time() - training_started_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Train Loss': fit_report.history['loss'], 'Validation Loss': fit_report.history['val_loss']})\\\n",
    "    .to_csv(f'{RESOURCES_PATH}/model_checkpoint/turnover/nn/mean_fc_nn/history.tsv', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = pd.read_csv(f'{RESOURCES_PATH}/model_checkpoint/turnover/nn/mean_fc_nn/history.tsv', sep='\\t')\n",
    "\n",
    "history.plot()\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_report(y_true_onehot, y_pred_proba):\n",
    "    y_true = y_true_onehot.argmax(axis=1)\n",
    "    y_pred = y_pred_proba.argmax(axis=1)\n",
    "\n",
    "    report = {}\n",
    "\n",
    "    report['accuracy'] = accuracy_score(y_true, y_pred)\n",
    "    report['log_loss'] = log_loss(y_true, y_pred_proba)\n",
    "\n",
    "    if 'confusion_matrix' in ADDITIONAL_REPORT_METRICS:\n",
    "        report['confusion_matrix'] = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_to_original_dataset_size(y_pred_proba):\n",
    "    original_y_size_diff = y_original_test.shape[1] - y_original_pred_proba.shape[1]\n",
    "    return np.pad(y_original_pred_proba, ((0, 0), (0, original_y_size_diff)), 'constant', constant_values=(0, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(f'{RESOURCES_PATH}/model_checkpoint/turnover/nn/mean_fc_nn/model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = model.predict(x_test, workers=WORKERS)\n",
    "y_original_pred_proba = model.predict(x_original_test, workers=WORKERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = {\n",
    "    'cleared_test': get_report(y_test, y_pred_proba),\n",
    "    'original_test': get_report(y_original_test, expand_to_original_dataset_size(y_original_pred_proba)),\n",
    "    'epochs': len(history),\n",
    "    'training_time_sec': training_time_sec\n",
    "}\n",
    "\n",
    "with open('report.json', 'w') as fout:\n",
    "    json.dump(report, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}